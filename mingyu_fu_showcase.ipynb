{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277cf67b",
   "metadata": {},
   "source": [
    "\n",
    "# AI Skill Showcase — Tennis Project (Mingyu Fu)\n",
    "My name is Mingyu Fu, I am currrently 1A student in honour mathematics. I have lots of experience on machine learning, such as number recognition system, tennis\n",
    "player motion detection, natural language model, etc. I pick the one that I have done before, which is tennis player detection, this project is aiming to category\n",
    "the motion of tennis player in either \"Hitting\" or \"Other\". Cause the data was collected manually, so the first step is to do something on the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da157a9",
   "metadata": {},
   "source": [
    "\n",
    "## Environment\n",
    "\n",
    "### If running on **Windwos || Linux**, consider installing following below:\n",
    "```python\n",
    "pip install numpy opencv-python pillow tqdm moviepy yolov5 torch torchvision\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a98ec3",
   "metadata": {},
   "source": [
    "## Step 1 Detect / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a5c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25573\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\25573/.cache\\torch\\hub\\master.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2025-9-19 Python-3.13.5 torch-2.8.0+cpu CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████| 14.1M/14.1M [00:01<00:00, 11.2MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "Using cache found in C:\\Users\\25573/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-9-19 Python-3.13.5 torch-2.8.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class PicProcess():             #class for piacture preprocessing\n",
    "    def __init__(self, model_path='ultralytics/yolov5', model_name='yolov5s'):\n",
    "        self.model = torch.hub.load(model_path, model_name, pretrained=True)           #download yolov5s model fast and small           \n",
    "    def Sequence2pic():\n",
    "        vc = cv2.VideoCapture(r\"D:/Tennis_Detect/Datasets(Vid2Pic)/RaW/202.mp4\")         #vc stores the content of video\n",
    "        n = 1  # count\n",
    "        timeF = 10  # pic per fps (shannon rule)\n",
    "        if vc.isOpened():\n",
    "            rval, frame = vc.read()     #rval true if open succefully, frame store data\n",
    "        else:\n",
    "            rval = False\n",
    "        i = 0\n",
    "        while rval:\n",
    "            rval, frame = vc.read()    #update rval status\n",
    "            if (n % timeF == 0):  # per timeF fps store\n",
    "                i += 1\n",
    "                print(i)\n",
    "                cv2.imwrite(r\"D:/Tennis_Detect/Datasets(Vid2Pic)/RaW/2pic{}.jpg\".format(i), frame)  # store\n",
    "            n = n + 1\n",
    "        vc.release()\n",
    "\n",
    "    def pic2HalfGrey():\n",
    "        #path = \"C:\\\\Users\\\\25573\\\\Desktop\\\\Program\\\\Tennis_Player_Detect\\\\Datasets(Vid2Pic)\"\n",
    "        #files = os.listdir(path)\n",
    "        i=int(1)\n",
    "        for i in range(3996):\n",
    "            if  os.path.exists(\"D:/Tennis_Detect/Datasets(Vid2Pic)/RaW/2pic\"+str(i)+\".jpg\") == False:\n",
    "                next\n",
    "            else:\n",
    "                img = Image.open(\"D:/Tennis_Detect/Datasets(Vid2Pic)/RaW/2pic\"+str(i)+\".jpg\")\n",
    "                width, height = img.size\n",
    "                #img = img.convert('L')\n",
    "                box = (0,height/2,width,986)       #986 because of audience may detected by yolo\n",
    "                img = img.crop(box)\n",
    "                img.save(\"D:/Tennis_Detect/Datasets(Vid2Pic)/After_Cut/Rf2Pic\"+str(i)+\".jpg\")\n",
    "                print(\"pic\"+str(i)+\"complete refine!\")\n",
    "    def Cut_Out_Athele(self, input_folder, output_folder):\n",
    "        Path(output_folder).mkdir(parents=True, exist_ok=True)      #make sure every time run will replaced old picture\n",
    "        for img_file in Path(input_folder).glob('*.jpg'):   #only seek the jpg picture in this directory\n",
    "            image = cv2.imread(str(img_file))           #__str__\n",
    "            if image is None:\n",
    "                continue\n",
    "            results = self.model(image)\n",
    "            for i, det in enumerate(results.xyxy[0]):       #det = [x1, y1, x2, y2, conf, cls]\n",
    "                if results.names[int(det[5])] == 'person':\n",
    "                    xmin, ymin, xmax, ymax = map(int, det[:4])\n",
    "                    cropped_img = image[ymin:ymax, xmin:xmax]\n",
    "                    save_path = os.path.join(output_folder, f'{img_file.stem}_person_{i}.jpg')    #frame012.jpg → frame012\n",
    "                    cv2.imwrite(save_path, cropped_img)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PicProcess.pic2HalfGrey()\n",
    "\n",
    "    input_folder = 'D:/Tennis_Detect/Datasets(Vid2Pic)/After_Cut/Temp'\n",
    "    output_folder = 'D:/Tennis_Detect/Datasets(Vid2Pic)/After_Cut/Temp1'\n",
    "    yolo = PicProcess()\n",
    "    yolo.Cut_Out_Athele(input_folder, output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92d6a3",
   "metadata": {},
   "source": [
    "## Step 2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
    "#image size:224**224\n",
    "\n",
    "train_dir = 'D:/Tennis_Detect/Datasets/Group(1) Picset/(1)ReadyPredictData(Hitting,no)/trial'\n",
    "test_dir = 'D:/Tennis_Detect/Datasets/Group(1) Picset/(1)ReadyPredictData(Hitting,no)/test'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = models.resnet50(pretrained=True)    #converge faster\n",
    "num_ftrs = model.fc.in_features        #usually have 1000 but 2 now\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "weights = torch.tensor([2.0, 1.0], device=device)      #train data unbalanced so 2\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5) #1e-5 avoid overfittng\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) #time 0.1 per 5 epoch\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()        #stop dropout\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1) #take the most possible one\n",
    "            all_preds.extend(predicted.cpu().numpy())   #flat in cpu as numpy only on cpu\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    return accuracy\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "    test_accuracy = evaluate_model(model, test_loader, device)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: D:/Program/RfPic2_person_0.jpg\n",
      "Pred: no_hitting  (confidence=0.9999)\n",
      "All probs: {'no_hitting': 0.9999, 'hitting': 0.0001}\n",
      "Image: D:/Program/RfPic168_person_0.jpg\n",
      "Pred: hitting  (confidence=1.0000)\n",
      "All probs: {'no_hitting': 0.0, 'hitting': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "MODEL_WEIGHTS = 'D:/Program/Pytorch_Status_Detect.pth'\n",
    "CLASS_NAMES = [\"no_hitting\", \"hitting\"] #0，1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def load_model(weights_path: str):\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "    state = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(model, img_path: str):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0).to(device)  # [1,3,224,224] add batch dimension\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1).squeeze(0).cpu().tolist()    #0~1\n",
    "    pred_idx = int(torch.argmax(logits, dim=1).item())\n",
    "    pred_label = CLASS_NAMES[pred_idx] if pred_idx < len(CLASS_NAMES) else str(pred_idx) ##\n",
    "    return pred_label, probs[pred_idx], probs \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    IMG_PATH = r\"D:/Program/RfPic2_person_0.jpg\"\n",
    "    model = load_model(MODEL_WEIGHTS)\n",
    "    label, conf, all_probs = predict_image(model, IMG_PATH)\n",
    "    print(f\"Image: {IMG_PATH}\")\n",
    "    print(f\"Pred: {label}  (confidence={conf:.4f})\")\n",
    "    print(\"All probs:\", {CLASS_NAMES[i]: round(p, 4) for i, p in enumerate(all_probs)})\n",
    "\n",
    "    IMG_PATH = r\"D:/Program/RfPic168_person_0.jpg\"\n",
    "    label, conf, all_probs = predict_image(model, IMG_PATH)\n",
    "    print(f\"Image: {IMG_PATH}\")\n",
    "    print(f\"Pred: {label}  (confidence={conf:.4f})\")\n",
    "    print(\"All probs:\", {CLASS_NAMES[i]: round(p, 4) for i, p in enumerate(all_probs)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9d32e",
   "metadata": {},
   "source": [
    "\n",
    "## Results & Discussion\n",
    "- As a result, this model could succefully detect the tennis player either is hitting the ball or not hitting the ball, which help tennis player to evaulate their\n",
    "performance after match.\n",
    "\n",
    "<div style=\"display:grid; grid-template-columns: repeat(2, 1fr); gap:14px; align-items:start; max-width:300px;\">\n",
    "  <figure style=\"margin:0; padding:10px; border:1px solid #e5e7eb; border-radius:14px; box-shadow:0 2px 10px rgba(0,0,0,.04);\">\n",
    "    <figcaption style=\"font-weight:600; margin-bottom:8px; font-size:14px;\">Expected: 1 · Output: 1</figcaption>\n",
    "    <img src=\"./RfPic168_person_0.jpg\" style=\"width:260px; height:auto; border-radius:10px; display:block;\">\n",
    "  </figure>\n",
    "  <figure style=\"margin:0; padding:10px; border:1px solid #e5e7eb; border-radius:14px; box-shadow:0 2px 10px rgba(0,0,0,.04);\">\n",
    "    <figcaption style=\"font-weight:600; margin-bottom:8px; font-size:14px;\">Expected: 0 · Output: 0</figcaption>\n",
    "    <img src=\"./RfPic2_person_0.jpg\" style=\"width:260px; height:auto; border-radius:10px; display:block;\">\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "The full concept of the project was describe in an article: https://doi.org/10.54254/2755-2721/2025.21197. You can see the origin source code in: https://github.com/Jerry-code-oss/Luck_Program/tree/main/Tennis_Detect/Code\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
