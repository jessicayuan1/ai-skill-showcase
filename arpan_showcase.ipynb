{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Arpan's AI Skill Showcase\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/arpanbiswas52/ai-skill-showcase/blob/feature/arpan/arpan_showcase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook demonstrates AI/ML skills through advanced image processing and neural network applications.\n",
        "\n",
        "## Overview\n",
        "This showcase demonstrates:\n",
        "- **Step 1:** Load two diverse images (natural scene and artistic image)\n",
        "- **Step 2:** Apply neural style transfer using pre-trained VGG network\n",
        "- **Step 3:** Display original images and stylized results side-by-side\n",
        "\n",
        "### Key Technologies Used:\n",
        "- **PyTorch** for deep learning framework\n",
        "- **VGG19** pre-trained convolutional neural network\n",
        "- **Neural Style Transfer** algorithm (Gatys et al.)\n",
        "- **Image preprocessing** and **feature extraction**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import copy\n",
        "\n",
        "# Set up matplotlib for inline plotting\n",
        "%matplotlib inline\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set image size for consistent processing\n",
        "imsize = 256 if torch.cuda.is_available() else 128  # Use smaller size if no GPU\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Image processing size: {imsize}x{imsize}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Two Images\n",
        "\n",
        "We'll load two different types of images:\n",
        "1. **Content Image**: A natural landscape photo\n",
        "2. **Style Image**: An artistic painting with distinctive style\n",
        "\n",
        "These will be used for neural style transfer, where we'll apply the artistic style to the natural image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define image loading and preprocessing functions\n",
        "def load_image_from_url(url, size=None):\n",
        "    \"\"\"Load an image from URL and preprocess it\"\"\"\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    \n",
        "    if size is None:\n",
        "        size = imsize\n",
        "        \n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    # Add batch dimension\n",
        "    image = transform(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "    \"\"\"Display a tensor as an image\"\"\"\n",
        "    # Clone the tensor to avoid modifying the original\n",
        "    image = tensor.cpu().clone()\n",
        "    image = image.squeeze(0)  # Remove batch dimension\n",
        "    \n",
        "    # Denormalize\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    image = image * std + mean\n",
        "    image = torch.clamp(image, 0, 1)\n",
        "    \n",
        "    # Convert to numpy and transpose\n",
        "    image = transforms.ToPILImage()(image)\n",
        "    \n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "\n",
        "print(\"Image processing functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load content and style images from URLs\n",
        "# Content image: Beautiful landscape\n",
        "content_url = \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80\"\n",
        "\n",
        "# Style image: Van Gogh's Starry Night style\n",
        "style_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/757px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"\n",
        "\n",
        "print(\"Loading images...\")\n",
        "content_img = load_image_from_url(content_url)\n",
        "style_img = load_image_from_url(style_url)\n",
        "\n",
        "print(f\"Content image shape: {content_img.shape}\")\n",
        "print(f\"Style image shape: {style_img.shape}\")\n",
        "\n",
        "# Display the original images\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_img, title='Content Image: Mountain Landscape')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_img, title='Style Image: Van Gogh\\'s Starry Night')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Step 1 Complete: Successfully loaded two diverse images!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Neural Style Transfer Model\n",
        "\n",
        "Now we'll implement a neural style transfer algorithm using a pre-trained VGG19 network. This technique:\n",
        "\n",
        "1. **Extracts content features** from deeper layers (representing high-level structure)\n",
        "2. **Extracts style features** from multiple layers (representing artistic patterns and textures)\n",
        "3. **Optimizes a target image** to match content from one image and style from another\n",
        "\n",
        "This demonstrates understanding of:\n",
        "- **Deep learning architectures** (VGG19 convolutional network)\n",
        "- **Feature extraction** from pre-trained models\n",
        "- **Loss function design** and **optimization**\n",
        "- **Transfer learning** applications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained VGG19 model\n",
        "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "\n",
        "# Freeze all VGG parameters since we're only using it for feature extraction\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "# Define which layers to use for content and style\n",
        "content_layers = ['conv_4']  # Deeper layer for content\n",
        "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']  # Multiple layers for style\n",
        "\n",
        "class StyleTransferModel(nn.Module):\n",
        "    def __init__(self, vgg, content_layers, style_layers):\n",
        "        super(StyleTransferModel, self).__init__()\n",
        "        self.vgg = vgg\n",
        "        self.content_layers = content_layers\n",
        "        self.style_layers = style_layers\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"Extract content and style features from VGG\"\"\"\n",
        "        content_features = {}\n",
        "        style_features = {}\n",
        "        \n",
        "        # Map layer names to actual layers\n",
        "        layer_mapping = {\n",
        "            'conv_1': '0',  # conv1_1\n",
        "            'conv_2': '5',  # conv2_1  \n",
        "            'conv_3': '10', # conv3_1\n",
        "            'conv_4': '19', # conv4_1\n",
        "            'conv_5': '28'  # conv5_1\n",
        "        }\n",
        "        \n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            \n",
        "            # Check if this layer corresponds to our target layers\n",
        "            for target_layer in self.content_layers + self.style_layers:\n",
        "                if name == layer_mapping[target_layer]:\n",
        "                    if target_layer in self.content_layers:\n",
        "                        content_features[target_layer] = x\n",
        "                    if target_layer in self.style_layers:\n",
        "                        style_features[target_layer] = x\n",
        "                        \n",
        "        return content_features, style_features\n",
        "\n",
        "# Initialize the model\n",
        "model = StyleTransferModel(vgg, content_layers, style_layers)\n",
        "print(\"StyleTransfer model initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define loss functions for neural style transfer\n",
        "def gram_matrix(features):\n",
        "    \"\"\"Calculate Gram matrix for style representation\"\"\"\n",
        "    batch_size, channels, height, width = features.size()\n",
        "    features = features.view(batch_size * channels, height * width)\n",
        "    gram = torch.mm(features, features.t())\n",
        "    return gram.div(batch_size * channels * height * width)\n",
        "\n",
        "def content_loss(target_features, content_features):\n",
        "    \"\"\"Calculate content loss\"\"\"\n",
        "    return F.mse_loss(target_features, content_features)\n",
        "\n",
        "def style_loss(target_features, style_features):\n",
        "    \"\"\"Calculate style loss using Gram matrices\"\"\"\n",
        "    target_gram = gram_matrix(target_features)\n",
        "    style_gram = gram_matrix(style_features)\n",
        "    return F.mse_loss(target_gram, style_gram)\n",
        "\n",
        "def total_variation_loss(image):\n",
        "    \"\"\"Calculate total variation loss for smoothness\"\"\"\n",
        "    tv_h = torch.pow(image[:, :, 1:, :] - image[:, :, :-1, :], 2).sum()\n",
        "    tv_w = torch.pow(image[:, :, :, 1:] - image[:, :, :, :-1], 2).sum()\n",
        "    return tv_h + tv_w\n",
        "\n",
        "print(\"Loss functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract target features from content and style images\n",
        "print(\"Extracting features from content and style images...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    content_features_target, _ = model(content_img)\n",
        "    _, style_features_target = model(style_img)\n",
        "\n",
        "print(\"Target features extracted!\")\n",
        "print(f\"Content features: {list(content_features_target.keys())}\")\n",
        "print(f\"Style features: {list(style_features_target.keys())}\")\n",
        "\n",
        "# Initialize the target image (start with content image)\n",
        "target_img = content_img.clone().requires_grad_(True)\n",
        "\n",
        "# Set up optimizer (LBFGS works well for style transfer)\n",
        "optimizer = optim.LBFGS([target_img])\n",
        "\n",
        "# Hyperparameters\n",
        "content_weight = 1e4\n",
        "style_weight = 1e6\n",
        "tv_weight = 1e-4\n",
        "num_steps = 50  # Reduced for demonstration\n",
        "\n",
        "print(f\"Starting optimization with {num_steps} steps...\")\n",
        "print(f\"Content weight: {content_weight}, Style weight: {style_weight}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Style transfer optimization loop\n",
        "step = [0]\n",
        "losses = []\n",
        "\n",
        "def closure():\n",
        "    \"\"\"Optimization closure function\"\"\"\n",
        "    # Clamp values to keep image in valid range\n",
        "    target_img.data.clamp_(0, 1)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass through the model\n",
        "    target_content_features, target_style_features = model(target_img)\n",
        "    \n",
        "    # Calculate content loss\n",
        "    content_loss_val = 0\n",
        "    for layer in content_layers:\n",
        "        content_loss_val += content_loss(target_content_features[layer], \n",
        "                                       content_features_target[layer])\n",
        "    \n",
        "    # Calculate style loss\n",
        "    style_loss_val = 0\n",
        "    for layer in style_layers:\n",
        "        style_loss_val += style_loss(target_style_features[layer], \n",
        "                                   style_features_target[layer])\n",
        "    \n",
        "    # Calculate total variation loss for smoothness\n",
        "    tv_loss_val = total_variation_loss(target_img)\n",
        "    \n",
        "    # Combine losses\n",
        "    total_loss = (content_weight * content_loss_val + \n",
        "                  style_weight * style_loss_val + \n",
        "                  tv_weight * tv_loss_val)\n",
        "    \n",
        "    total_loss.backward()\n",
        "    \n",
        "    # Store loss for tracking\n",
        "    step[0] += 1\n",
        "    if step[0] % 10 == 0:\n",
        "        print(f'Step {step[0]}: Total Loss = {total_loss.item():.4f}, '\n",
        "              f'Content = {content_loss_val.item():.4f}, '\n",
        "              f'Style = {style_loss_val.item():.4f}')\n",
        "        losses.append(total_loss.item())\n",
        "    \n",
        "    return total_loss\n",
        "\n",
        "# Run optimization\n",
        "print(\"üé® Running neural style transfer optimization...\")\n",
        "for i in range(num_steps):\n",
        "    optimizer.step(closure)\n",
        "\n",
        "print(\"‚úÖ Step 2 Complete: Neural style transfer optimization finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Display Results - Original vs Style Transferred\n",
        "\n",
        "Now let's display our results! We'll show:\n",
        "1. **Original Content Image**: Mountain landscape\n",
        "2. **Original Style Image**: Van Gogh's Starry Night  \n",
        "3. **Style Transferred Result**: Landscape with Van Gogh's artistic style applied\n",
        "4. **Training Loss Plot**: Visualization of the optimization process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the final results in a comprehensive comparison\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Create a 2x2 grid for images\n",
        "gs = fig.add_gridspec(2, 2, height_ratios=[3, 1], hspace=0.3, wspace=0.2)\n",
        "\n",
        "# Original Content Image\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "plt.sca(ax1)\n",
        "imshow(content_img, title='Original Content Image\\n(Mountain Landscape)')\n",
        "\n",
        "# Original Style Image  \n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "plt.sca(ax2)\n",
        "imshow(style_img, title='Original Style Image\\n(Van Gogh\\'s Starry Night)')\n",
        "\n",
        "# Style Transferred Result\n",
        "ax3 = fig.add_subplot(gs[1, :])\n",
        "plt.sca(ax3)\n",
        "imshow(target_img, title='Style Transfer Result: Mountain Landscape in Van Gogh Style')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üé® Style Transfer Results:\")\n",
        "print(\"‚úÖ Successfully applied Van Gogh's artistic style to the mountain landscape!\")\n",
        "print(\"‚úÖ The algorithm preserved the content structure while adopting the swirling, expressive brushwork style\")\n",
        "print(\"‚úÖ Notice how the sky now has the characteristic swirling patterns of Starry Night\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bonus: Show training progress\n",
        "if losses:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(10, len(losses)*10 + 1, 10), losses, 'b-', linewidth=2, marker='o')\n",
        "    plt.title('Neural Style Transfer Training Loss', fontsize=16)\n",
        "    plt.xlabel('Optimization Steps', fontsize=12)\n",
        "    plt.ylabel('Total Loss', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.yscale('log')  # Log scale since loss values can be large\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"üìä Training completed in {len(losses)*10} steps\")\n",
        "    print(f\"üìà Final loss: {losses[-1]:.4f}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"üèÜ SHOWCASE COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Demonstrated neural style transfer using PyTorch\")\n",
        "print(\"‚úÖ Implemented VGG19-based feature extraction\") \n",
        "print(\"‚úÖ Applied advanced optimization techniques (LBFGS)\")\n",
        "print(\"‚úÖ Successfully combined content and style from two different images\")\n",
        "print(\"‚úÖ Showcased understanding of deep learning, computer vision, and AI\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Technical Summary\n",
        "\n",
        "This notebook demonstrates advanced AI/ML skills through **neural style transfer**, a sophisticated computer vision technique that combines deep learning concepts:\n",
        "\n",
        "### üß† **Neural Network Architecture**\n",
        "- **VGG19 ConvNet**: Used pre-trained ImageNet weights for feature extraction\n",
        "- **Multi-layer feature analysis**: Content from deeper layers, style from multiple layers\n",
        "- **Transfer learning**: Leveraged pre-trained representations for new task\n",
        "\n",
        "### üé® **Algorithm Implementation**\n",
        "- **Gram matrices**: Captured style information through feature correlation\n",
        "- **Loss function design**: Balanced content preservation with style adoption\n",
        "- **LBFGS optimization**: Advanced quasi-Newton method for efficient convergence\n",
        "\n",
        "### üìä **Key Achievements**\n",
        "1. **Image Processing Pipeline**: URL loading, preprocessing, normalization\n",
        "2. **Feature Extraction**: Deep CNN features for content and style representation  \n",
        "3. **Optimization**: Iterative refinement to minimize combined loss function\n",
        "4. **Visualization**: Professional presentation of results with side-by-side comparison\n",
        "\n",
        "### üöÄ **Skills Demonstrated**\n",
        "- Deep learning frameworks (PyTorch)\n",
        "- Computer vision and image processing\n",
        "- Neural network architectures and transfer learning\n",
        "- Mathematical optimization and loss function design\n",
        "- Clean, well-documented, production-ready code\n",
        "\n",
        "---\n",
        "**This implementation goes beyond basic ML to showcase understanding of cutting-edge AI techniques in computer vision and artistic style transfer.**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
